{"block_file": {"data_exporters/taxi_to_gcs_parquet.py:data_exporter:python:taxi to gcs parquet": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'mage_zoomcamp_beau_branton_bucket'\n    object_key = 'nyc_taxi_data.parquet'\n\n    GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        bucket_name,\n        object_key,\n    )\n", "file_path": "data_exporters/taxi_to_gcs_parquet.py", "language": "python", "type": "data_exporter", "uuid": "taxi_to_gcs_parquet"}, "data_exporters/intriguing_frost.py:data_exporter:python:intriguing frost": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'your_bucket_name'\n    object_key = 'your_object_key'\n\n    GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        bucket_name,\n        object_key,\n    )\n", "file_path": "data_exporters/intriguing_frost.py", "language": "python", "type": "data_exporter", "uuid": "intriguing_frost"}, "data_exporters/green_taxi_to_big_query_hw_3.py:data_exporter:python:green taxi to big query hw 3": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n\n    table_id = 'peppy-citron-411704.ny_taxi.green_taxi_hw_week_3'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/green_taxi_to_big_query_hw_3.py", "language": "python", "type": "data_exporter", "uuid": "green_taxi_to_big_query_hw_3"}, "data_exporters/write_taxi_to_bigquery.sql:data_exporter:sql:write taxi to bigquery": {"content": "select *\nfrom {{ df_1 }}", "file_path": "data_exporters/write_taxi_to_bigquery.sql", "language": "sql", "type": "data_exporter", "uuid": "write_taxi_to_bigquery"}, "data_exporters/taxi_data_to_postgres.py:data_exporter:python:taxi data to postgres": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    schema_name = 'ny_taxi'  # Specify the name of the schema to export data to\n    table_name = 'yellow_cab_data'  # Specify the name of the table to export data to\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'dev'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            schema_name,\n            table_name,\n            index=False,  # Specifies whether to include index in exported table\n            if_exists='replace',  # Specify resolution policy if table name already exists\n        )\n", "file_path": "data_exporters/taxi_data_to_postgres.py", "language": "python", "type": "data_exporter", "uuid": "taxi_data_to_postgres"}, "data_exporters/export_taxi_to_gcp_parameter.py:data_exporter:python:export taxi to gcp parameter": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n    now = kwargs.get('execution_date')\n    # print(now)\n    # print(now.strftime(\"%Y-%m-%d\"))\n    now_fpath = now.strftime(\"%Y-%m-%d\")\n\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'mage_zoomcamp_beau_branton_bucket'\n    object_key = f'Daily_Trips/{now_fpath}_nyc_taxi_daily_trips.parquet'\n    print(object_key)\n\n    GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        bucket_name,\n        object_key,\n    )\n", "file_path": "data_exporters/export_taxi_to_gcp_parameter.py", "language": "python", "type": "data_exporter", "uuid": "export_taxi_to_gcp_parameter"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/green_taxi_to_postgres.py:data_exporter:python:green taxi to postgres": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    schema_name = 'mage'  # Specify the name of the schema to export data to\n    table_name = 'green_taxi'  # Specify the name of the table to export data to\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'dev'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            schema_name,\n            table_name,\n            index=False,  # Specifies whether to include index in exported table\n            if_exists='replace',  # Specify resolution policy if table name already exists\n        )\n", "file_path": "data_exporters/green_taxi_to_postgres.py", "language": "python", "type": "data_exporter", "uuid": "green_taxi_to_postgres"}, "data_exporters/green_taxi_to_gcs_parquet.py:data_exporter:python:green taxi to gcs parquet": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nfrom os import path\nimport os\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"./peppy-citron-411704-888e6b9e01ee.json\"\n\nbucket_name = 'mage_zoomcamp_beau_branton_bucket'\nproject_id = 'peppy-citron-411704'\n\ntable_name = \"green_taxi_data\"\nroot_path = f'{bucket_name}/{table_name}'\n\n\n\n\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n\n    table = pa.Table.from_pandas(data)\n\n    gcs = pa.fs.GcsFileSystem()\n    pq.write_to_dataset(\n        table,\n        root_path=root_path,\n        partition_cols=['lpep_pickup_date'],\n        filesystem=gcs\n   )", "file_path": "data_exporters/green_taxi_to_gcs_parquet.py", "language": "python", "type": "data_exporter", "uuid": "green_taxi_to_gcs_parquet"}, "data_exporters/export_green_taxi_hw_week_3.py:data_exporter:python:export green taxi hw week 3": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'de_zoomcamp_test_bucket'\n    object_key = 'green_taxi_data_week_3_hw.parquet'\n\n    GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        bucket_name,\n        object_key,\n    )\n", "file_path": "data_exporters/export_green_taxi_hw_week_3.py", "language": "python", "type": "data_exporter", "uuid": "export_green_taxi_hw_week_3"}, "data_exporters/taxi_to_gcs_partition.py:data_exporter:python:taxi to gcs partition": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nfrom os import path\nimport os\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"./peppy-citron-411704-888e6b9e01ee.json\"\n\nbucket_name = 'mage_zoomcamp_beau_branton_bucket'\nproject_id = 'peppy-citron-411704'\n\ntable_name = \"nyc_taxi_data\"\nroot_path = f'{bucket_name}/{table_name}'\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    data['tpep_pickup_datetime'] = data['tpep_pickup_datetime'].dt.date\n    data['tpep_dropoff_datetime'] = data['tpep_dropoff_datetime'].dt.date\n    table = pa.Table.from_pandas(data)\n\n    gcs = pa.fs.GcsFileSystem()\n    pq.write_to_dataset(\n        table,\n        root_path=root_path,\n        partition_cols=['tpep_pickup_date'],\n        filesystem=gcs\n   )", "file_path": "data_exporters/taxi_to_gcs_partition.py", "language": "python", "type": "data_exporter", "uuid": "taxi_to_gcs_partition"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/load_green_taxi.py:data_loader:python:load green taxi": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n\n    dates = ['2020-10','2020-11','2020-12']\n    base_url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_{}.csv.gz'\n    green_taxi_dtypes = {\n                        'VendorID': pd.Int64Dtype(),\n                        'lpep_pickup_datetime': str,\n                        'lpep_dropoff_datetime': str,\n                        'store_and_fwd_flag':\tstr,\n                        'RatecodeID':\tpd.Int64Dtype(),\n                        'PULocationID': pd.Int64Dtype(),\n                        'DOLocationID': pd.Int64Dtype(),\n                        'passenger_count': pd.Int64Dtype(),\n                        'trip_distance': float,\n                        'fare_amount': float,\n                        'extra':\tfloat,   \n                        'mta_tax':\tfloat,\n                        'tip_amount':\tfloat,\n                        'tolls_amount': float,\t\n                        'ehail_fee':\tfloat,\n                        'improvement_surcharge': float,\t\n                        'total_amount': float,\t\n                        'payment_type': pd.Int64Dtype(),\t\n                        'trip_type': pd.Int64Dtype(),\t\n                        'congestion_surcharge': float\n    }\n\n    # native date parsing \n    parse_dates = ['lpep_pickup_datetime', 'lpep_dropoff_datetime']\n    dfs = []\n    for date in dates:\n        url = base_url.format(date)\n        print(url)\n        df = pd.read_csv(url, sep=',', compression='gzip', dtype=green_taxi_dtypes, parse_dates=parse_dates)\n        dfs.append(df)\n    output = pd.concat(dfs,ignore_index=True)\n    return output\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_green_taxi.py", "language": "python", "type": "data_loader", "uuid": "load_green_taxi"}, "data_loaders/load_taxi_data.sql:data_loader:sql:load taxi data": {"content": "SELECT * \nFROM  ny_taxi.yellow_cab_data \nLIMIT 10\n", "file_path": "data_loaders/load_taxi_data.sql", "language": "sql", "type": "data_loader", "uuid": "load_taxi_data"}, "data_loaders/test_postgres.sql:data_loader:sql:test postgres": {"content": "SELECT 1;\n", "file_path": "data_loaders/test_postgres.sql", "language": "sql", "type": "data_loader", "uuid": "test_postgres"}, "data_loaders/test_gcs.py:data_loader:python:test gcs": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'mage_zoomcamp_beau_branton_bucket'\n    object_key = 'titanic_clean.csv'\n\n    return GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(\n        bucket_name,\n        object_key,\n    )\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/test_gcs.py", "language": "python", "type": "data_loader", "uuid": "test_gcs"}, "data_loaders/load_taxi_gcs.py:data_loader:python:load taxi gcs": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'mage_zoomcamp_beau_branton_bucket'\n    object_key = 'nyc_taxi_data.parquet'\n\n\n    return GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(\n        bucket_name,\n        object_key,\n    )", "file_path": "data_loaders/load_taxi_gcs.py", "language": "python", "type": "data_loader", "uuid": "load_taxi_gcs"}, "data_loaders/load_green_taxi_week_3_hw.py:data_loader:python:load green taxi week 3 hw": {"content": "import io\nimport pandas as pd\nimport requests\nimport pyarrow.parquet as pq\nfrom io import BytesIO\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n\n\n    url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-{}.parquet'\n    \n    #Drastically reduces memory by defining data types\n    #Should fail if the data type changes\n    dtypes = {\n                'VendorID': 'int64',\n                'lpep_pickup_datetime': 'datetime64[ns]',\n                'lpep_dropoff_datetime': 'datetime64[ns]',\n                'store_and_fwd_flag': 'object',\n                'RatecodeID': 'float64',\n                'PULocationID': 'int64',\n                'DOLocationID': 'int64',\n                'passenger_count': 'float64',\n                'trip_distance': 'float64',\n                'fare_amount': 'float64',\n                'extra': 'float64',\n                'mta_tax': 'float64',\n                'tip_amount': 'float64',\n                'tolls_amount': 'float64',\n                'ehail_fee': 'object',\n                'improvement_surcharge': 'float64',\n                'total_amount': 'float64',\n                'payment_type': 'float64',\n                'trip_type': 'float64',\n                'congestion_surcharge': 'float64'\n    }\n\n    dfs = []\n    for i in range(1, 13):\n        date = str(i).zfill(2)\n        print(date)\n        url_chunk = url.format(date)\n        print(url_chunk)\n\n        response = requests.get(url_chunk)\n        response.raise_for_status()  # Raise exception if invalid response\n\n        # Load parquet data into a pandas dataframe\n        df = pq.read_table(BytesIO(response.content)).to_pandas()\n        dfs.append(df)\n        output = pd.concat(dfs,ignore_index=True)\n        print(output['lpep_pickup_datetime'].tail())\n\n    return output\n\n\n    #returns dataframe as url with dtype = taxi_dtypes defined above, also knows to parse the above two columns as dates\n    # return pd.read_csv(\n    #     url, sep=',', compression='gzip', dtype=taxi_dtypes, parse_dates=parse_dates\n    #     )\n\n\n# @test\n# def test_output(output, *args) -> None:\n#     \"\"\"\n#     Template code for testing the output of the block.\n#     \"\"\"\n#     assert output is not None, 'The output is undefined'", "file_path": "data_loaders/load_green_taxi_week_3_hw.py", "language": "python", "type": "data_loader", "uuid": "load_green_taxi_week_3_hw"}, "data_loaders/load_api_data.py:data_loader:python:load api data": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz'\n    \n    #Drastically reduces memory by defining data types\n    #Should fail if the data type changes\n    taxi_dtypes = {\n                    'VendorID': pd.Int64Dtype(),\n                    'passenger_count': pd.Int64Dtype(),\n                    'trip_distance': float,\n                    'RatecodeID':pd.Int64Dtype(),\n                    'store_and_fwd_flag':str,\n                    'PULocationID':pd.Int64Dtype(),\n                    'DOLocationID':pd.Int64Dtype(),\n                    'payment_type': pd.Int64Dtype(),\n                    'fare_amount': float,\n                    'extra':float,\n                    'mta_tax':float,\n                    'tip_amount':float,\n                    'tolls_amount':float,\n                    'improvement_surcharge':float,\n                    'total_amount':float,\n                    'congestion_surcharge':float\n                }\n\n    # native date parsing \n    parse_dates = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n\n    #returns dataframe as url with dtype = taxi_dtypes defined above, also knows to parse the above two columns as dates\n    return pd.read_csv(\n        url, sep=',', compression='gzip', dtype=taxi_dtypes, parse_dates=parse_dates\n        )\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "data_loaders/load_api_data.py", "language": "python", "type": "data_loader", "uuid": "load_api_data"}, "transformers/transform_staged_data.py:transformer:python:transform staged data": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    data.columns = (data.columns\n                    .str.replace(' ','_')\n                    .str.lower()\n    )\n    data['tpep_pickup_datetime'] = data['tpep_pickup_datetime'].dt.date\n    data['tpep_dropoff_datetime'] = data['tpep_dropoff_datetime'].dt.date\n    return data\n", "file_path": "transformers/transform_staged_data.py", "language": "python", "type": "transformer", "uuid": "transform_staged_data"}, "transformers/curious_tree.py:transformer:python:curious tree": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/curious_tree.py", "language": "python", "type": "transformer", "uuid": "curious_tree"}, "transformers/transform_green_taxi_hw_3.py:transformer:python:transform green taxi hw 3": {"content": "import pandas as pd\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    # print(data.dtypes)\n    # data['lpep_pickup_datetime'] = pd.to_datetime(data['lpep_pickup_datetime'], errors='coerce')\n    # data['lpep_dropoff_datetime'] = pd.to_datetime(data['lpep_pickup_datetime'], errors='coerce')\n    data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt.date\n    data['lpep_dropoff_date'] = data['lpep_pickup_datetime'].dt.date\n    data = data.drop(columns=['lpep_pickup_datetime', 'lpep_dropoff_datetime'])\n    print(data.dtypes)\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform_green_taxi_hw_3.py", "language": "python", "type": "transformer", "uuid": "transform_green_taxi_hw_3"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_age = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_age)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/transform_green_taxi.py:transformer:python:transform green taxi": {"content": "import re\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef camel_to_snake(name):\n    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name)\n    return name.lower().lstrip('_')\n\n@transformer\ndef transform(data, *args, **kwargs):\n\n    data = data[(data['passenger_count'] != 0) & (data['trip_distance'] != 0)] #Assumes trip distance and passenger count must be >= 0\n    # print( len(data[data['passenger_count'] == 0 ]))\n    data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt.date\n    data.columns = [camel_to_snake(col) for col in data.columns]\n    # print(data.columns)\n    \n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n\n    assert 'vendor_id' in output.columns, 'The column vendor_id does not exist'\n    assert (output['passenger_count']>0).all(), 'Some rows have passenger_count >= 0'\n    assert (output['trip_distance']>0).all(), 'Some rows have trip_distance >= 0'\n", "file_path": "transformers/transform_green_taxi.py", "language": "python", "type": "transformer", "uuid": "transform_green_taxi"}, "transformers/transform_taxi_data.py:transformer:python:transform taxi data": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n#simple filter on data where passenger_count > 0\n@transformer\ndef transform(data, *args, **kwargs):\n   \n   print(f\"Preprocessing { data['passenger_count'].isin([0]).sum() } rows with zero passengers\")\n   return data[data['passenger_count'] > 0]\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output['passenger_count'].isin([0]).sum() == 0, 'There are no rides with zero passengers'\n", "file_path": "transformers/transform_taxi_data.py", "language": "python", "type": "transformer", "uuid": "transform_taxi_data"}, "transformers/transform_green_taxi_hw_week_3.py:transformer:python:transform green taxi hw week 3": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n   \n   ## Need to return datetime columns as date time \n   ##Run other necessary transformations - but check hw.md!!!\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform_green_taxi_hw_week_3.py", "language": "python", "type": "transformer", "uuid": "transform_green_taxi_hw_week_3"}, "/home/src/magic-zoomcamp/data_loaders/scrape_daily_box_scores.py:data_loader:python:home/src/magic-zoomcamp/data loaders/scrape daily box scores": {"content": "from bs4 import BeautifulSoup\nimport requests\nimport time\nfrom datetime import datetime\nimport pandas as pd\nimport re\nfrom requests import Session\nfrom datetime import datetime, timedelta\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# GET request options\n_HEADERS = {'User-Agent': 'Mozilla/5.0'}\n\n#Open webpage and locate home and away tables to scrape boxstats\ndef get_stats(url):\n    with Session() as s:\n        r = s.get('https://stats.ncaa.org'+url, headers=_HEADERS)\n    if r.status_code == 403:\n        print('An error occurred with the GET Request')\n        print('403 Error: NCAA blocked request')\n    soup = BeautifulSoup(r.text, features='lxml')\n    tables = soup.find_all(\"table\")\n\n    away_table = tables[5]\n    home_table = tables[6]\n\n    away_df = scrape_box_stats(away_table)\n    home_df = scrape_box_stats(home_table)\n\n    return away_df, home_df\n\ndef scrape_box_stats(table):\n    headers = [header.text for header in table.find('tr', {'class': 'grey_heading'}).find_all('th')]        \n    data = []\n    for row in table.find_all('tr', {'class': 'smtext'}):\n        data.append([cell.text.strip().split('/')[0] for cell in row.find_all('td')])  # Split at '/' for dirty data and take the first part\n    df = pd.DataFrame(data, columns=headers)\n    df = df.replace('', 0)\n    return df\n\ndef get_team_names(table_html):\n    # Parse the HTML content\n    soup = BeautifulSoup(table_html, 'html.parser')\n\n    # Find the table rows\n    rows = soup.find_all('tr')\n\n    # Get the team names and remove the record in parentheses\n    away_team_name = rows[1].find('a').text.split(' (')[0].strip()\n    home_team_name = rows[2].find('a').text.split(' (')[0].strip()\n\n    return {\"Away Team\": away_team_name, \"Home Team\": home_team_name}\n\nfrom bs4 import BeautifulSoup\n\ndef get_game_info(table_html):\n    # Parse the HTML content\n    soup = BeautifulSoup(table_html, 'html.parser')\n\n    # Find the table rows\n    rows = soup.find_all('tr')\n    # Get the game date, location, and attendance (if the data is available)\n    if len(rows)>=3:     \n        game_date = rows[0].find_all('td')[1].text.strip()\n        location = rows[1].find_all('td')[1].text.strip()\n        attendance = rows[2].find_all('td')[1].text.strip()\n    elif len(rows) == 2:\n        # Case: len(rows) = 2\n        game_date = rows[0].find_all('td')[1].text.strip()\n        location = rows[1].find_all('td')[1].text.strip()\n        attendance = None\n    elif len(rows) == 1:\n        # Case: len(rows) = 1\n        game_date = rows[0].find_all('td')[1].text.strip()\n        location = None\n        attendance = None\n    else:\n        # Case: len(rows) = 0\n        game_date, location, attendance = None, None, None\n\n    return {\"Game Date\": game_date, \"Location\": location, \"Attendance\": attendance}\n\ndef get_box_score_links(html):\n    # Parse the HTML content\n    soup = BeautifulSoup(html, 'html.parser')\n    # Find all the 'a' tags\n    a_tags = soup.find_all('a')\n    # Get the links that contain \"box_score\"\n    links = [a['href'] for a in a_tags if \"box_score\" in a['href']]\n    return links\n\n@data_loader\ndef load_data(*args, **kwargs):\n\n    # Start the timer\n    start_time = time.time()\n\n    # Define variables for ingestion script\n    sport_code = \"MBA\"\n    academic_year = \"2024\"\n    division = \"1\"\n    start_date_str = \"03/04/2024\"\n    end_date_str = \"03/04/2024\"\n\n    start_date = datetime.strptime(start_date_str, \"%m/%d/%Y\")\n    end_date = datetime.strptime(end_date_str, \"%m/%d/%Y\")\n    date_range = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n\n    # Print the date range\n    for game_date in date_range:\n        \n        # Construct the base URL\n        base_url = \"https://stats.ncaa.org/contests/livestream_scoreboards\"\n        \n        # Create a dictionary of parameters\n        params = {\n            \"utf8\": \"\u2713\",\n            \"sport_code\": sport_code,\n            \"academic_year\": academic_year,\n            \"division\": division,\n            \"game_date\": game_date,\n        }\n        \n        with Session() as s:\n            r = s.get(base_url, params=params, headers=_HEADERS)\n        if r.status_code == 403:\n            print('An error occurred with the GET Request')\n            print('403 Error: NCAA blocked request')\n        \n        # Create empty dataframes for final stats\n        final_batting_df = pd.DataFrame()\n        final_pitching_df = pd.DataFrame()\n        final_fielding_df = pd.DataFrame()\n        final_games_per_day = pd.DataFrame()\n        \n        #Retrieve all links to game box score webpages\n        game_links = get_box_score_links(r.text)\n        num_games = 0    \n        for game in game_links:\n            num_games+=1\n            url = 'https://stats.ncaa.org'+game\n            print(f\"Getting stats from: {url}\")\n            \n            #------------------------GET URLS FOR BATTING,PITCHING,FIELDING\n            with Session() as s:\n                r = s.get(url, headers=_HEADERS)\n            if r.status_code == 403:\n                print('An error occurred with the GET Request')\n                print('403 Error: NCAA blocked request')\n            soup = BeautifulSoup(r.text, features='lxml')\n            \n            # Find the table element that contains the player data\n            tables = soup.find_all(\"table\")\n            \n            team_names = get_team_names(tables[0].prettify())\n            game_info = get_game_info(tables[2].prettify())\n            stat_type = tables[4].find_all('a')\n            \n            \n            #Store URLs for batting, pitching and fielding box scores\n            urls = [stat_type[0]['href'], stat_type[1]['href'], stat_type[2]['href']]\n\n            # Get the home and away stats for each stat type\n            for i, url in enumerate(urls):\n                away_df, home_df = get_stats(url)  \n                ##MAKE INTO FUNCTION\n                away_df['game_id'] =  int(re.search(r'\\d+', game).group())\n                home_df['game_id'] = int(re.search(r'\\d+', game).group())\n                away_df['team'] = team_names[\"Away Team\"]\n                home_df['team'] = team_names[\"Home Team\"]\n                away_df['date'] = game_date\n                home_df['date'] = game_date\n                away_df['location'] = game_info[\"Location\"]\n                home_df['location'] = game_info[\"Location\"]\n                away_df['attendance'] = game_info[\"Attendance\"]\n                home_df['attendance'] = game_info[\"Attendance\"]\n                \n                # Append away and home stats to the final dataframe\n                if i == 0:  # Batting stats\n                    final_batting_df = pd.concat([final_batting_df, away_df, home_df])\n                elif i == 1:  # Pitching stats\n                    final_pitching_df = pd.concat([final_pitching_df, away_df, home_df])\n                elif i == 2:  # Fielding stats\n                    final_fielding_df = pd.concat([final_fielding_df, away_df, home_df])\n\n        print(f\"Finished scraping data from {num_games} games on {game_date}\")\n        new_row = pd.DataFrame({\"Date\": [game_date], \"num_games\": [num_games]})\n        final_games_per_day = pd.concat([final_games_per_day, new_row], ignore_index=True)\n        \n    # End the timer and print the elapsed time\n    end_time = time.time()\n    print(f\"Time taken: {end_time - start_time} seconds\")\n\n    return final_batting_df, final_pitching_df, final_fielding_df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/magic-zoomcamp/data_loaders/scrape_daily_box_scores.py", "language": "python", "type": "data_loader", "uuid": "scrape_daily_box_scores"}, "/home/src/magic-zoomcamp/transformers/transform_pitching_box_scores.py:transformer:python:home/src/magic-zoomcamp/transformers/transform pitching box scores": {"content": "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    final_pitching_df = df[1]\n    #TRANSFORM DATA (CASTING TO CORRECT DTYPES etc.)\n    pitching_int_cols = ['G', 'App', 'GS', 'CG', 'H', 'R', 'ER', 'BB', 'SO', 'SHO', 'BF', 'P-OAB', '2B-A', '3B-A', 'Bk', 'HR-A', 'WP', 'HB', 'IBB', 'Inh Run', 'Inh Run Score', 'SHA', 'SFA', 'Pitches', 'GO', 'FO', 'W', 'L', 'SV', 'OrdAppeared', 'KL', 'pickoffs','game_id','attendance']\n    for col in pitching_int_cols:\n        final_pitching_df[col] = np.floor(pd.to_numeric(final_pitching_df[col], errors='coerce')).astype('Int64')\n    \n\n    final_pitching_df['ingestion_date'] = datetime.now()\n    final_pitching_df = final_pitching_df.astype({\n        'Player': str,\n        'Pos': str,\n        'team': str,\n        'location': str,\n        'date': 'datetime64[ns]',\n        'ingestion_date': 'datetime64[ns]'\n    })\n    # final_pitching_df['IP'] = np.floor(pd.to_numeric(final_pitching_df['IP'], errors='coerce')).astype('float64')\n\n    return final_pitching_df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "/home/src/magic-zoomcamp/transformers/transform_pitching_box_scores.py", "language": "python", "type": "transformer", "uuid": "transform_pitching_box_scores"}, "/home/src/magic-zoomcamp/transformers/transform_batting_box_scores.py:transformer:python:home/src/magic-zoomcamp/transformers/transform batting box scores": {"content": "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    final_batting_df = df[0]\n    #TRANSFORM DATA (CASTING TO CORRECT DTYPES etc.)\n    batting_int_cols = ['G', 'R', 'AB', 'H', '2B', '3B', 'TB', 'HR', 'RBI', 'BB', 'HBP', 'SF', 'SH', 'K', 'OPP DP', 'CS', 'Picked', 'SB', 'IBB', 'GDP', 'RBI2out','game_id','attendance']\n    # pitching_int_cols = ['G', 'App', 'GS', 'CG', 'H', 'R', 'ER', 'BB', 'SO', 'SHO', 'BF', 'P-OAB', '2B-A', '3B-A', 'Bk', 'HR-A', 'WP', 'HB', 'IBB', 'Inh Run', 'Inh Run Score', 'SHA', 'SFA', 'Pitches', 'GO', 'FO', 'W', 'L', 'SV', 'OrdAppeared', 'KL', 'pickoffs','game_id','attendance']\n    # fielding_int_cols = ['G','PO','A','TC','E','CI','PB','SBA','CSB','IDP','TP','game_id','attendance']\n\n    for col in batting_int_cols:   \n        final_batting_df[col] = np.floor(pd.to_numeric(final_batting_df[col], errors='coerce')).astype('Int64')\n    # for col in pitching_int_cols:\n    #     final_pitching_df[col] = np.floor(pd.to_numeric(final_pitching_df[col], errors='coerce')).astype('Int64')\n    # for col in fielding_int_cols:\n    #     final_fielding_df[col] = np.floor(pd.to_numeric(final_fielding_df[col], errors='coerce')).astype('Int64')\n\n    final_batting_df['ingestion_date'] = datetime.now()\n    final_batting_df = final_batting_df.astype({\n        'Player': str,\n        'Pos': str,\n        'team': str,\n        'location': str,\n        'date': 'datetime64[ns]',\n        'ingestion_date': 'datetime64[ns]'\n    })\n    # final_pitching_df['IP'] = np.floor(pd.to_numeric(final_pitching_df['IP'], errors='coerce')).astype('float64')\n\n    return final_batting_df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "/home/src/magic-zoomcamp/transformers/transform_batting_box_scores.py", "language": "python", "type": "transformer", "uuid": "transform_batting_box_scores"}, "/home/src/magic-zoomcamp/transformers/transform_fielding_box_scores.py:transformer:python:home/src/magic-zoomcamp/transformers/transform fielding box scores": {"content": "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nhttp://localhost:6789/pipelines/ncaa_baseball_daily_box_score_test/edit?sideview=tree#\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    final_fielding_df = df[2]\n    #TRANSFORM DATA (CASTING TO CORRECT DTYPES etc.)\n    fielding_int_cols = ['G','PO','A','TC','E','CI','PB','SBA','CSB','IDP','TP','game_id','attendance']\n\n    for col in fielding_int_cols:\n        final_fielding_df[col] = np.floor(pd.to_numeric(final_fielding_df[col], errors='coerce')).astype('Int64')\n\n    final_fielding_df['ingestion_date'] = datetime.now()\n    final_fielding_df = final_fielding_df.astype({\n        'Player': str,\n        'Pos': str,\n        'team': str,\n        'location': str,\n        'date': 'datetime64[ns]',\n        'ingestion_date': 'datetime64[ns]'\n    })\n    # final_pitching_df['IP'] = np.floor(pd.to_numeric(final_pitching_df['IP'], errors='coerce')).astype('float64')\n\n    return final_fielding_df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "/home/src/magic-zoomcamp/transformers/transform_fielding_box_scores.py", "language": "python", "type": "transformer", "uuid": "transform_fielding_box_scores"}, "/home/src/magic-zoomcamp/data_exporters/export_batting_box_scores_to_gcs.py:data_exporter:python:home/src/magic-zoomcamp/data exporters/export batting box scores to gcs": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nimport pandas as pd\nfrom os import path\nimport os\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n###CONFUSED ABOUT HOW THIS CAN SCRIPT CAN BE UPDATED SO THAT SOMEONE CAN REPLICATE THIS PROJECT. WILL THEY SETUP THEIR OWN GCS AND UPDATE THESE CREDENTIALS?\n#HOW CAN I MAKE THESE CREDENTIALS VARIABLES THAT THE END USER CAN UPDATE\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"./peppy-citron-411704-888e6b9e01ee.json\"\n\nbucket_name = 'mage_zoomcamp_beau_branton_bucket'\nproject_id = 'peppy-citron-411704'\n\ntable_name = \"batting_box_scores_test\"\nroot_path = f'{bucket_name}/d1_baseball_project/{table_name}'\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n\n    table = pa.Table.from_pandas(df)\n\n    gcs = pa.fs.GcsFileSystem()\n    pq.write_to_dataset(\n        table,\n        root_path=root_path,\n        partition_cols=['date'],\n        filesystem=gcs\n   )\n", "file_path": "/home/src/magic-zoomcamp/data_exporters/export_batting_box_scores_to_gcs.py", "language": "python", "type": "data_exporter", "uuid": "export_batting_box_scores_to_gcs"}, "/home/src/magic-zoomcamp/data_exporters/export_pitching_box_scores_gcs.py:data_exporter:python:home/src/magic-zoomcamp/data exporters/export pitching box scores gcs": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nimport pandas as pd\nfrom os import path\nimport os\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n###CONFUSED ABOUT HOW THIS CAN SCRIPT CAN BE UPDATED SO THAT SOMEONE CAN REPLICATE THIS PROJECT. WILL THEY SETUP THEIR OWN GCS AND UPDATE THESE CREDENTIALS?\n#HOW CAN I MAKE THESE CREDENTIALS VARIABLES THAT THE END USER CAN UPDATE\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"./peppy-citron-411704-888e6b9e01ee.json\"\n\nbucket_name = 'mage_zoomcamp_beau_branton_bucket'\nproject_id = 'peppy-citron-411704'\n\ntable_name = \"pitching_box_scores_test\"\nroot_path = f'{bucket_name}/d1_baseball_project/{table_name}'\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n\n    table = pa.Table.from_pandas(df)\n\n    gcs = pa.fs.GcsFileSystem()\n    pq.write_to_dataset(\n        table,\n        root_path=root_path,\n        partition_cols=['date'],\n        filesystem=gcs\n   )\n", "file_path": "/home/src/magic-zoomcamp/data_exporters/export_pitching_box_scores_gcs.py", "language": "python", "type": "data_exporter", "uuid": "export_pitching_box_scores_gcs"}, "/home/src/magic-zoomcamp/data_exporters/export_fielding_box_scores_gcs.py:data_exporter:python:home/src/magic-zoomcamp/data exporters/export fielding box scores gcs": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nimport pandas as pd\nfrom os import path\nimport os\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n###CONFUSED ABOUT HOW THIS CAN SCRIPT CAN BE UPDATED SO THAT SOMEONE CAN REPLICATE THIS PROJECT. WILL THEY SETUP THEIR OWN GCS AND UPDATE THESE CREDENTIALS?\n#HOW CAN I MAKE THESE CREDENTIALS VARIABLES THAT THE END USER CAN UPDATE\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"./peppy-citron-411704-888e6b9e01ee.json\"\n\nbucket_name = 'mage_zoomcamp_beau_branton_bucket'\nproject_id = 'peppy-citron-411704'\n\ntable_name = \"fielding_box_scores_test\"\nroot_path = f'{bucket_name}/d1_baseball_project/{table_name}'\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n\n    table = pa.Table.from_pandas(df)\n\n    gcs = pa.fs.GcsFileSystem()\n    pq.write_to_dataset(\n        table,\n        root_path=root_path,\n        partition_cols=['date'],\n        filesystem=gcs\n   )\n", "file_path": "/home/src/magic-zoomcamp/data_exporters/export_fielding_box_scores_gcs.py", "language": "python", "type": "data_exporter", "uuid": "export_fielding_box_scores_gcs"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}